{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Evaluation Notebook — AQ-NBEATS Exog+Series Model\n",
    "\n",
    "Adapted from the paper's `Evaluate.ipynb` for `AnyQuantileForecasterExogWithSeries`.\n",
    "\n",
    "**Changes vs the original notebook:**\n",
    "- Loads `AnyQuantileForecasterExogWithSeries` instead of `AnyQuantileForecaster`\n",
    "- Uses `EMHIRESUnivariateDataModule` (has `series_id` and exog support)\n",
    "- Checkpoint pattern matches `nbeatsaq-exog-series` naming convention\n",
    "- Computes CRPS on all 11 fixed eval quantiles (no random/deterministic split needed)\n",
    "- Optionally loads and applies the isotonic calibrator from `calibrate.py`\n",
    "- Saves per-series results in the same pickle format as the original for downstream compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from model.models import AnyQuantileForecasterExogWithSeries\n",
    "from dataset.datasets import EMHIRESUnivariateDataModule\n",
    "from metrics import SMAPE, MAPE, CRPS\n",
    "from utils.model_factory import instantiate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg_header",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Checkpoint discovery ───────────────────────────────────────────────────────\n",
    "EXPERIMENT_NAME  = \"nbeatsaq-exog-series\"   \n",
    "CHECKPOINT_NAME  = \"model-epoch=13.ckpt\"    # or \"*\" to pick all epochs\n",
    "\n",
    "checkpoint_pattern = (\n",
    "    f\"lightning_logs/{EXPERIMENT_NAME}*/checkpoints/{CHECKPOINT_NAME}\"\n",
    ")\n",
    "\n",
    "# ── Eval quantiles (fixed grid, same as plot_interactive.py) ──────────────────\n",
    "EVAL_QUANTILES = [0.01, 0.05, 0.10, 0.25, 0.40, 0.50, 0.60, 0.75, 0.90, 0.95, 0.99]\n",
    "\n",
    "# ── Calibrator (optional — set to None to skip) ───────────────────────────────\n",
    "CALIBRATOR_PATH = \"results/calibration/calibrator.pkl\"  # or None\n",
    "\n",
    "# ── Dataset split (must match training config) ────────────────────────────────\n",
    "SPLIT_BOUNDARIES = [\"2006-01-01\", \"2017-12-30\", \"2018-01-01\", \"2019-01-01\"]\n",
    "\n",
    "model_list = sorted(glob(checkpoint_pattern))\n",
    "print(f\"Pattern : {checkpoint_pattern}\")\n",
    "print(f\"Found   : {len(model_list)} checkpoint(s)\")\n",
    "for m in model_list:\n",
    "    print(f\"  {m}\")\n",
    "\n",
    "if not model_list:\n",
    "    raise FileNotFoundError(f\"No checkpoints found: {checkpoint_pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {},
   "source": [
    "## 2. Load Config & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path(\"config/nbeatsaq-exog-series.yaml\")\n",
    "\n",
    "def load_cfg(config_path: Path):\n",
    "    \"\"\"Load OmegaConf cfg, stripping the !!python/tuple YAML tag.\"\"\"\n",
    "    if config_path.exists():\n",
    "        with open(config_path) as f:\n",
    "            raw = f.read().replace(\"!!python/tuple\", \"\")\n",
    "        return OmegaConf.create(yaml.safe_load(raw))\n",
    "    # Fallback minimal config\n",
    "    return OmegaConf.create({\n",
    "        \"model\": {\n",
    "            \"input_horizon_len\": 168, \"max_norm\": True,\n",
    "            \"num_series\": 35, \"series_embed_dim\": 32, \"series_embed_scale\": 0.08,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"name\": \"MHLV\", \"train_batch_size\": 512, \"eval_batch_size\": 512,\n",
    "            \"num_workers\": 0, \"persistent_workers\": False,\n",
    "            \"horizon_length\": 24, \"history_length\": 168,\n",
    "            \"split_boundaries\": SPLIT_BOUNDARIES,\n",
    "            \"fillna\": \"ffill\", \"train_step\": 1, \"eval_step\": 24,\n",
    "        },\n",
    "    })\n",
    "\n",
    "cfg = load_cfg(CONFIG_PATH)\n",
    "cfg.dataset.split_boundaries = SPLIT_BOUNDARIES  # override if needed\n",
    "\n",
    "dm = EMHIRESUnivariateDataModule(\n",
    "    name               = cfg.dataset.name,\n",
    "    train_batch_size   = cfg.dataset.train_batch_size,\n",
    "    eval_batch_size    = cfg.dataset.eval_batch_size,\n",
    "    num_workers        = 0,\n",
    "    persistent_workers = False,\n",
    "    horizon_length     = cfg.dataset.horizon_length,\n",
    "    history_length     = cfg.dataset.history_length,\n",
    "    split_boundaries   = cfg.dataset.split_boundaries,\n",
    "    fillna             = cfg.dataset.fillna,\n",
    "    train_step         = cfg.dataset.train_step,\n",
    "    eval_step          = cfg.dataset.eval_step,\n",
    ")\n",
    "dm.setup(stage=\"test\")\n",
    "test_loader = dm.test_dataloader()\n",
    "print(f\"Test samples: {len(dm.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with one checkpoint\n",
    "test_ckpt = model_list[0]\n",
    "print(f\"Testing checkpoint: {test_ckpt}\")\n",
    "\n",
    "# Add weights_only=False to bypass the security check\n",
    "model_test = AnyQuantileForecasterExogWithSeries.load_from_checkpoint(\n",
    "    test_ckpt, \n",
    "    cfg=cfg, \n",
    "    strict=False, \n",
    "    map_location='cpu',\n",
    "    weights_only=False  # ← ADD THIS LINE\n",
    ")\n",
    "model_test.eval()\n",
    "\n",
    "# Get one batch\n",
    "test_batch = next(iter(test_loader))\n",
    "test_batch['quantiles'] = torch.full((test_batch['history'].shape[0], 1), 0.5, dtype=torch.float32)\n",
    "\n",
    "# Try forward\n",
    "try:\n",
    "    output = model_test(test_batch)\n",
    "    print(f\"✅ Forward works! Output shape: {output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Forward failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect_header",
   "metadata": {},
   "source": [
    "## 3. Collect Ground-Truth Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror the original notebook's metadata collection loop\n",
    "dfs = []\n",
    "for b in tqdm(test_loader, desc=\"Collecting metadata\"):\n",
    "    row = {}\n",
    "    for k in [\"target\", \"history\", \"series_id\"]:\n",
    "        if k in b:\n",
    "            row[k] = list(b[k].cpu().numpy())\n",
    "    dfs.append(pd.DataFrame.from_dict(row))\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "print(f\"Total rows : {len(df)}\")\n",
    "print(f\"Series IDs : {sorted(df.series_id.unique().astype(int))}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pred_header",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions (all seeds → ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6_predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_checkpoint(ckpt_path: str, cfg, dataloader, quantiles: list, device):\n",
    "    \"\"\"\n",
    "    Load one checkpoint and produce predictions at all fixed quantiles.\n",
    "    Returns tensor of shape [N, H, Q].\n",
    "    \"\"\"\n",
    "    model = AnyQuantileForecasterExogWithSeries.load_from_checkpoint(\n",
    "        ckpt_path, \n",
    "        cfg=cfg, \n",
    "        strict=False, \n",
    "        map_location=device,\n",
    "        weights_only=False  # ← ADD THIS\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"  batches\", leave=False):\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        B = batch[\"history\"].shape[0]\n",
    "        per_q = []\n",
    "        \n",
    "        for q_val in quantiles:\n",
    "            batch[\"quantiles\"] = torch.full((B, 1), q_val, device=device, dtype=torch.float32)\n",
    "            pred = model(batch)\n",
    "            \n",
    "            if pred.dim() == 3:\n",
    "                per_q.append(pred.squeeze(-1).cpu())\n",
    "            else:\n",
    "                per_q.append(pred.cpu())\n",
    "        \n",
    "        all_preds.append(torch.stack(per_q, dim=-1))\n",
    "    \n",
    "    return torch.cat(all_preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9af9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup safe globals ONCE before loading any checkpoints\n",
    "import torch.serialization\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "from omegaconf.base import ContainerMetadata\n",
    "import typing\n",
    "import collections\n",
    "\n",
    "# Add ALL necessary safe globals\n",
    "torch.serialization.add_safe_globals([\n",
    "    # OmegaConf types\n",
    "    DictConfig, \n",
    "    ListConfig, \n",
    "    ContainerMetadata,\n",
    "    \n",
    "    # Python built-ins\n",
    "    int,\n",
    "    float,\n",
    "    str,\n",
    "    bool,\n",
    "    list,\n",
    "    dict,\n",
    "    tuple,\n",
    "    set,\n",
    "    \n",
    "    # Typing\n",
    "    typing.Any,\n",
    "    \n",
    "    # Collections\n",
    "    collections.defaultdict,\n",
    "    collections.OrderedDict,\n",
    "])\n",
    "\n",
    "print(\"✅ Safe globals configured\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "per_seed_preds = []\n",
    "for i, ckpt in enumerate(model_list):\n",
    "    print(f\"\\n[{i+1}/{len(model_list)}] {Path(ckpt).parent.parent.name}\")\n",
    "    try:\n",
    "        preds = predict_checkpoint(ckpt, cfg, test_loader, EVAL_QUANTILES, device)\n",
    "        per_seed_preds.append(preds)\n",
    "        print(f\"  ✅ Shape: {preds.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error loading checkpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "# Ensemble: average across seeds\n",
    "predictions_ensemble = torch.stack(per_seed_preds).mean(dim=0)\n",
    "predictions_ensemble, _ = torch.sort(predictions_ensemble, dim=-1)\n",
    "print(f\"\\n✅ Ensemble predictions shape: {predictions_ensemble.shape}\")\n",
    "print(f\"   [N_samples, H_horizon, Q_quantiles] = {predictions_ensemble.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "calib_header",
   "metadata": {},
   "source": [
    "## 5. (Optional) Apply Isotonic Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7_calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import calibrator class BEFORE loading pickle\n",
    "from calibrate import IsotonicCalibrator\n",
    "\n",
    "calibrator = None\n",
    "predictions_calibrated = predictions_ensemble.clone()\n",
    "\n",
    "if CALIBRATOR_PATH and Path(CALIBRATOR_PATH).exists():\n",
    "    with open(CALIBRATOR_PATH, \"rb\") as f:\n",
    "        calibrator = pickle.load(f)\n",
    "    print(f\"✅ Calibrator loaded from {CALIBRATOR_PATH}\")\n",
    "\n",
    "    # Get corrected raw quantiles and re-query the model\n",
    "    corrected_q = calibrator.corrected_query_quantiles(EVAL_QUANTILES)\n",
    "    print(f\"   Nominal  : {[f'{q:.3f}' for q in EVAL_QUANTILES]}\")\n",
    "    print(f\"   Corrected: {[f'{q:.3f}' for q in corrected_q]}\")\n",
    "\n",
    "    cal_preds_list = []\n",
    "    for i, ckpt in enumerate(model_list):\n",
    "        print(f\"\\n  Re-querying [{i+1}/{len(model_list)}] at corrected quantiles…\")\n",
    "        preds = predict_checkpoint(ckpt, cfg, test_loader, corrected_q.tolist(), device)\n",
    "        cal_preds_list.append(preds)\n",
    "\n",
    "    predictions_calibrated = torch.stack(cal_preds_list).mean(dim=0)\n",
    "    predictions_calibrated, _ = torch.sort(predictions_calibrated, dim=-1)\n",
    "    print(f\"\\nCalibrated ensemble shape: {predictions_calibrated.shape}\")\n",
    "else:\n",
    "    print(\"ℹ️  No calibrator found — using raw predictions.\")\n",
    "    print(\"   Run calibrate.py with --save-calibrator to generate one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_header",
   "metadata": {},
   "source": [
    "## 6. CRPS & Coverage Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8_crps",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tensor = torch.tensor(EVAL_QUANTILES, dtype=torch.float32)\n",
    "\n",
    "def compute_all_metrics(preds_tensor, df_meta, quantiles_list, label=\"\"):\n",
    "    \"\"\"\n",
    "    Compute CRPS, per-quantile coverage, MAE, MAPE for a [N, H, Q] prediction tensor.\n",
    "    \"\"\"\n",
    "    q_t = torch.tensor(quantiles_list, dtype=torch.float32)\n",
    "    crps_metric  = CRPS()\n",
    "    mape_metric  = MAPE()\n",
    "    smape_metric = SMAPE()\n",
    "\n",
    "    preds_np  = preds_tensor.numpy()          # [N, H, Q]\n",
    "    targets   = np.array(list(df_meta.target))# [N, H]\n",
    "\n",
    "    # --- CRPS (using paper's metric class) ---\n",
    "    for i in tqdm(range(len(df_meta)), desc=f\"{label} CRPS\", leave=False):\n",
    "        tgt = torch.tensor(targets[i], dtype=torch.float32)\n",
    "        if torch.isinf(tgt).any() or torch.isnan(tgt).any():\n",
    "            continue\n",
    "        pred = torch.tensor(preds_np[i], dtype=torch.float32)   # [H, Q]\n",
    "        crps_metric.update(\n",
    "            preds=pred[None],      # [1, H, Q]\n",
    "            target=tgt[None],      # [1, H]\n",
    "            q=q_t[None],           # [1, Q]\n",
    "        )\n",
    "\n",
    "    # --- Point forecast metrics (median = Q0.50) ---\n",
    "    mid_idx    = quantiles_list.index(0.50) if 0.50 in quantiles_list else len(quantiles_list)//2\n",
    "    pred_median = torch.tensor(preds_np[:, :, mid_idx], dtype=torch.float32)  # [N, H]\n",
    "    target_t    = torch.tensor(targets, dtype=torch.float32)                   # [N, H]\n",
    "    valid       = ~(torch.isnan(target_t) | torch.isinf(target_t))\n",
    "    mape_metric.update(pred_median[valid], target_t[valid])\n",
    "    smape_metric.update(pred_median[valid], target_t[valid])\n",
    "\n",
    "    # --- Coverage per quantile ---\n",
    "    coverage = {}\n",
    "    for i, q in enumerate(quantiles_list):\n",
    "        pred_q = preds_np[:, :, i]  # [N, H]\n",
    "        hit    = np.mean(targets <= pred_q)\n",
    "        coverage[q] = hit\n",
    "\n",
    "    return {\n",
    "        \"crps\"    : crps_metric.compute().item(),\n",
    "        \"mape\"    : mape_metric.compute().item(),\n",
    "        \"smape\"   : smape_metric.compute().item(),\n",
    "        \"mae\"     : float(np.nanmean(np.abs(preds_np[:,:,mid_idx] - targets))),\n",
    "        \"coverage\": coverage,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Computing metrics on raw ensemble…\")\n",
    "metrics_raw = compute_all_metrics(predictions_ensemble, df, EVAL_QUANTILES, label=\"Raw\")\n",
    "\n",
    "if calibrator is not None:\n",
    "    print(\"\\nComputing metrics on calibrated ensemble…\")\n",
    "    metrics_cal = compute_all_metrics(predictions_calibrated, df, EVAL_QUANTILES, label=\"Cal\")\n",
    "else:\n",
    "    metrics_cal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9_print_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics, label=\"\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  CRPS  : {metrics['crps']:.3f} MW\")\n",
    "    print(f\"  MAE   : {metrics['mae']:.3f} MW\")\n",
    "    print(f\"  MAPE  : {metrics['mape']*100:.2f}%\")\n",
    "    print(f\"  sMAPE : {metrics['smape']*100:.2f}%\")\n",
    "    print(f\"\\n  {'Quantile':>10}  {'Coverage':>10}  {'Error':>10}\")\n",
    "    print(f\"  {'-'*34}\")\n",
    "    for q, cov in metrics[\"coverage\"].items():\n",
    "        print(f\"  {q:>10.3f}  {cov:>10.3f}  {cov-q:>+10.3f}\")\n",
    "    mace = np.mean([abs(cov - q) for q, cov in metrics[\"coverage\"].items()])\n",
    "    print(f\"\\n  MACE  : {mace:.4f}\")\n",
    "\n",
    "print_metrics(metrics_raw, \"RAW ENSEMBLE\")\n",
    "if metrics_cal:\n",
    "    print_metrics(metrics_cal, \"CALIBRATED ENSEMBLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ncrps_header",
   "metadata": {},
   "source": [
    "## 7. Normalised CRPS (N-CRPS) — Paper's Primary Metric\n",
    "\n",
    "Replicates Eq. (18) from the paper: normalises by mean country demand so all 35 series contribute equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s10_ncrps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ncrps(preds_np, df_meta, quantiles_list):\n",
    "    \"\"\"\n",
    "    N-CRPS = 100 * (1/C) * sum_c [ (1 / H*N_c*Q) * sum_{i,h,q} pinball(y,f) / y_bar_c ]\n",
    "    Replicates Eq. (18) in the paper.\n",
    "    \"\"\"\n",
    "    q_arr   = np.array(quantiles_list)\n",
    "    targets = np.array(list(df_meta.target))      # [N, H]\n",
    "    series  = df_meta.series_id.values\n",
    "    country_ncrps = {}\n",
    "\n",
    "    for sid in np.unique(series):\n",
    "        mask   = series == sid\n",
    "        tgt    = targets[mask]                    # [Nc, H]\n",
    "        pred   = preds_np[mask]                   # [Nc, H, Q]\n",
    "        y_bar  = np.nanmean(tgt)\n",
    "        if y_bar == 0:\n",
    "            continue\n",
    "\n",
    "        # pinball loss: [Nc, H, Q]\n",
    "        err = tgt[..., np.newaxis] - pred\n",
    "        pb  = np.where(err >= 0, q_arr * err, (q_arr - 1) * err)\n",
    "        country_ncrps[int(sid)] = pb.mean() / y_bar * 100\n",
    "\n",
    "    ncrps_all = list(country_ncrps.values())\n",
    "    return np.mean(ncrps_all), country_ncrps\n",
    "\n",
    "\n",
    "preds_np_raw = predictions_ensemble.numpy()\n",
    "ncrps_raw, ncrps_per_country_raw = compute_ncrps(preds_np_raw, df, EVAL_QUANTILES)\n",
    "print(f\"N-CRPS (raw ensemble)      : {ncrps_raw:.4f}\")\n",
    "\n",
    "if calibrator is not None:\n",
    "    preds_np_cal = predictions_calibrated.numpy()\n",
    "    ncrps_cal, ncrps_per_country_cal = compute_ncrps(preds_np_cal, df, EVAL_QUANTILES)\n",
    "    print(f\"N-CRPS (calibrated ensemble): {ncrps_cal:.4f}\")\n",
    "\n",
    "# Compare vs paper baseline (AQ-NBEATS: 211.22 CRPS, 1.84 N-CRPS)\n",
    "print(f\"\\nPaper AQ-NBEATS baseline   : N-CRPS 1.84\")\n",
    "print(f\"Your model (raw)           : N-CRPS {ncrps_raw:.4f}\")\n",
    "if calibrator:\n",
    "    print(f\"Your model (calibrated)    : N-CRPS {ncrps_cal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s11_per_country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-country N-CRPS breakdown\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "countries = sorted(ncrps_per_country_raw.keys())\n",
    "vals_raw  = [ncrps_per_country_raw[c] for c in countries]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "x = np.arange(len(countries))\n",
    "ax.bar(x, vals_raw, color=\"#2196F3\", alpha=0.8, label=\"Raw\")\n",
    "if calibrator:\n",
    "    vals_cal = [ncrps_per_country_cal[c] for c in countries]\n",
    "    ax.bar(x, vals_cal, color=\"#4CAF50\", alpha=0.6, label=\"Calibrated\")\n",
    "ax.axhline(1.84, color=\"red\", lw=1.5, ls=\"--\", label=\"Paper AQ-NBEATS (1.84)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(countries, rotation=45, fontsize=8)\n",
    "ax.set(xlabel=\"Series ID\", ylabel=\"N-CRPS\", title=\"Per-Country N-CRPS\")\n",
    "ax.legend(); ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/ncrps_per_country.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## 8. Save Per-Series Results (compatible with paper format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s12_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which predictions to save\n",
    "predictions_to_save = (\n",
    "    predictions_calibrated if calibrator is not None else predictions_ensemble\n",
    ")\n",
    "preds_np_save = predictions_to_save.numpy()  # [N, H, Q]\n",
    "\n",
    "RESULTS_PATH = os.path.join(RESULTS_DIR, f\"MHLV/ExogSeries-calibrated={calibrator is not None}\")\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "targets_all = np.array(list(df.target))      # [N, H]\n",
    "q_arr       = np.array(EVAL_QUANTILES)       # [Q]\n",
    "\n",
    "# Save one pickle per seed (compatibility with downstream scripts)\n",
    "for worker_idx, seed_preds in enumerate(per_seed_preds):\n",
    "    p = seed_preds.numpy()  # [N, H, Q]\n",
    "\n",
    "    for series_id in df.series_id.unique():\n",
    "        mask       = df.series_id == series_id\n",
    "        df_series  = df[mask]\n",
    "        p_series   = p[mask.values]              # [Nc, H, Q]\n",
    "\n",
    "        target_series = np.nan_to_num(targets_all[mask.values], posinf=np.nan)  # [Nc, H]\n",
    "        # Expand for compatibility: [Nc, H, Q]\n",
    "        target_rep    = np.repeat(target_series[..., None], len(EVAL_QUANTILES), axis=-1)\n",
    "        # Quantiles: [Nc, H, Q]\n",
    "        quant_rep     = np.broadcast_to(q_arr, p_series.shape).copy()\n",
    "\n",
    "        forec = pd.DataFrame({f\"forec{worker_idx+1}\": p_series.ravel()})\n",
    "        if worker_idx == 0:\n",
    "            forec[\"actuals\"] = target_rep.ravel()\n",
    "            forec[\"quants\"]  = quant_rep.ravel()\n",
    "            forec = forec[[\"actuals\", \"quants\", \"forec1\"]]\n",
    "\n",
    "        out_path = os.path.join(RESULTS_PATH, f\"e1w{worker_idx+1}_{int(series_id)}.pickle\")\n",
    "        forec.to_pickle(out_path)\n",
    "\n",
    "print(f\"✅ Results saved to {RESULTS_PATH}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 9. Final Summary vs Paper Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s13_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_baselines = {\n",
    "    \"Naive\"    : {\"crps\": 502.62, \"ncrps\": 3.95, \"mape\": 5.08},\n",
    "    \"ARIMA\"    : {\"crps\": 353.49, \"ncrps\": 2.83, \"mape\": 3.74},\n",
    "    \"ES\"       : {\"crps\": 325.80, \"ncrps\": 2.64, \"mape\": 3.48},\n",
    "    \"WaveNet\"  : {\"crps\": 293.38, \"ncrps\": 2.52, \"mape\": 3.39},\n",
    "    \"AQ-ESRNN\" : {\"crps\": 195.94, \"ncrps\": 1.72, \"mape\": 2.32},\n",
    "    \"AQ-NBEATS\": {\"crps\": 211.22, \"ncrps\": 1.84, \"mape\": 2.47},\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, v in paper_baselines.items():\n",
    "    rows.append({\"Model\": name, \"CRPS\": v[\"crps\"], \"N-CRPS\": v[\"ncrps\"], \"MAPE %\": v[\"mape\"]})\n",
    "\n",
    "rows.append({\n",
    "    \"Model\"  : \"Exog+Series (raw)\",\n",
    "    \"CRPS\"   : metrics_raw[\"crps\"],\n",
    "    \"N-CRPS\" : ncrps_raw,\n",
    "    \"MAPE %\" : metrics_raw[\"mape\"] * 100,\n",
    "})\n",
    "\n",
    "if metrics_cal:\n",
    "    rows.append({\n",
    "        \"Model\"  : \"Exog+Series (calibrated)\",\n",
    "        \"CRPS\"   : metrics_cal[\"crps\"],\n",
    "        \"N-CRPS\" : ncrps_cal,\n",
    "        \"MAPE %\" : metrics_cal[\"mape\"] * 100,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"Model\")\n",
    "summary_df = summary_df.sort_values(\"CRPS\")\n",
    "\n",
    "# Highlight our rows\n",
    "def highlight_ours(row):\n",
    "    return [\"background-color: #e8f5e9\" if \"Exog\" in row.name else \"\" for _ in row]\n",
    "\n",
    "display(summary_df.style.apply(highlight_ours, axis=1).format(\n",
    "    {\"CRPS\": \"{:.2f}\", \"N-CRPS\": \"{:.4f}\", \"MAPE %\": \"{:.2f}\"}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s14_save_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "summary_df.to_csv(f\"{RESULTS_DIR}/benchmark_summary.csv\")\n",
    "print(f\"✅ Summary saved to {RESULTS_DIR}/benchmark_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
