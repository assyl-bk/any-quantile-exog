# Smooth Pinball Loss Configuration (Sluijterman et al. 2024)
# Novel contribution: Smooth MQN loss with sigmoid indicator approximation
# 
# Key innovation:
# - Keeps MQNLoss structure exactly (maintains calibration)
# - Only smooths the indicator function 1(u < 0) → sigmoid(-u/s)
# - Non-zero second derivative for better optimization
# - Smooth gradients at y = ŷ
#
# Expected: Coverage ~0.95 (same as MQNLoss), CRPS 195-205 (3-8% improvement)

logging.path: "./lightning_logs/${dataset.name}"
logging.name: "smooth-mqn-${model.loss.smoothing_method}-h${dataset.history_length}-s${random.seed}"

dataset._target_: dataset.ElectricityUnivariateDataModule
dataset.name: "MHLV"
dataset.num_workers: 2  # Reduced to match system capabilities
dataset.persistent_workers: True
dataset.train_batch_size: 240  # Standard batch size
dataset.eval_batch_size: 240
dataset.horizon_length: 48
dataset.history_length: 168
dataset.split_boundaries:
  ["2006-01-01", "2017-01-01", "2018-01-01", "2019-01-01"]
dataset.fillna: "ffill"
dataset.train_step: 1
dataset.eval_step: 24

random.seed: !!python/tuple [0, 1, 2, 3, 4, 5]
trainer.max_epochs: 15
trainer.check_val_every_n_epoch: 1
trainer.log_every_n_steps: 100
trainer.devices: 1
trainer.accelerator: "gpu"
trainer.fast_dev_run: False
trainer.limit_train_batches: null
trainer.limit_val_batches: null

checkpoint.resume_ckpt: null
checkpoint.save_top_k: 5
checkpoint.ckpt_path: last

model._target_: model.AnyQuantileForecaster
model.input_horizon_len: "${dataset.history_length}"
model.max_norm: True
model.q_sampling: "random_in_batch"
model.q_distribution: "uniform"
model.q_parameter: 0.3

# Smooth MQN Loss (maintains MQNLoss structure, only smooths indicator)
model.loss._target_: losses.AdaptiveSmoothMQNLoss
model.loss.smoothing_method: "sigmoid"  # 'sigmoid', 'tanh', or 'arctan'
model.loss.initial_smoothness: 1.0  # Start moderately smooth
model.loss.final_smoothness: 0.1  # End sharper (closer to standard)
model.loss.reduction: "mean"

# Use proven NBEATSAQCAT architecture
model.nn.backbone._target_: modules.NBEATSAQCAT
model.nn.backbone.dropout: 0.0
model.nn.backbone.layer_width: 1024
model.nn.backbone.num_layers: 3
model.nn.backbone.num_blocks: 30
model.nn.backbone.share: False
model.nn.backbone.size_in: "${dataset.history_length}"
model.nn.backbone.size_out: "${dataset.horizon_length}"

model.optimizer._target_: torch.optim.Adam
model.optimizer.lr: 0.0005  # Standard baseline LR

model.scheduler._target_: schedulers.InverseSquareRoot
model.scheduler.warmup_updates: 400
model.scheduler.warmup_end_lr: "${model.optimizer.lr}"
