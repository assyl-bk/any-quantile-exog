# CQR Post-Processing Configuration
# Applies Conformalized Quantile Regression to NBEATSAQFILM baseline

random:
  seed:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
logging:
  path: ./lightning_logs/${dataset.name}
  name: model=NBEATSAQ-CQR-alpha${model.cqr_alpha}-history=${dataset.history_length}-lr=${model.optimizer.lr}-seed=${random.seed}

dataset:
  _target_: dataset.ElectricityUnivariateDataModule
  name: 'MHLV'
  history_length: 168
  horizon_length: 48
  train_batch_size: 1024
  eval_batch_size: 1024
  num_workers: 4
  persistent_workers: true
  split_boundaries: ['2006-01-01', '2017-12-30', '2018-01-01', '2019-01-01']
  fillna: ffill
  train_step: 1
  eval_step: 24

model:
  _target_: model.AnyQuantileForecasterCQR
  
  input_horizon_len: 168
  max_norm: true
  
  # CQR Configuration - Post-processing for guaranteed coverage
  cqr_alpha: 0.05  # 95% coverage
  
  # Standard MQNLoss (proven baseline)
  loss:
    _target_: losses.MQNLoss
    reduction: mean
  
  # Quantile sampling
  q_sampling: random_in_batch
  q_distribution: uniform
  q_parameter: 0.3
  
  nn:
    backbone:
      _target_: modules.NBEATSAQFILM
      num_blocks: 30
      num_layers: 3
      layer_width: 1024
      share: false
      size_in: 168
      size_out: 48
  
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0005
    weight_decay: 0.0001
  
  scheduler:
    _target_: schedulers.InverseSquareRoot
    warmup_steps: 400
    min_lr: 0.00001

trainer:
  max_epochs: 15
  check_val_every_n_epoch: 1
  log_every_n_steps: 100
  devices: 1
  accelerator: gpu
  precision: 32

checkpoint:
  resume_ckpt: last
  save_top_k: 5
  monitor: val/crps
  mode: min
  ckpt_path: last
