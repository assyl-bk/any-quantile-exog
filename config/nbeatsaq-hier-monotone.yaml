model:
  _target_: model.AnyQuantileForecasterWithHierarchicalMonotonicity
  nn:
    backbone:
      _target_: modules.NBEATSAQCAT
      num_blocks: 30
      num_layers: 3
      layer_width: 1024
      share: false
      size_in: 168
      size_out: 48  # Match horizon_length
      dropout: 0.0
      quantile_embed_dim: 64
      quantile_embed_num: 100
  
  input_horizon_len: 168
  
  loss:
    _target_: losses.MQNLoss
  
  # Quantile sampling - uniform like baseline that achieved CRPS=211
  q_sampling: random_in_batch
  q_distribution: uniform
  q_parameter: 0.3
  
  max_norm: true
  
  # Monotonicity parameters - DISABLED (weight=0) to match baseline
  monotone_weight: 0.0  # Disabled - pure backbone performance
  monotone_margin: 0.01
  num_train_quantiles: 1  # Use single quantile (like baseline)
  
  # Optimizer - baseline settings
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0005
    weight_decay: 0.000005
  
  scheduler:
    _target_: schedulers.InverseSquareRoot
    warmup_updates: 400
    warmup_end_lr: 0.0005

dataset:
  _target_: dataset.ElectricityUnivariateDataModule
  name: MHLV
  train_batch_size: 240  # Keep same as hierarchical config
  eval_batch_size: 240
  num_workers: 4
  persistent_workers: true
  split_boundaries: ['2006-01-01', '2017-01-01', '2018-01-01', '2019-01-01']
  history_length: 168
  horizon_length: 48  # Match size_out
  fillna: 'ffill'
  train_step: 1
  eval_step: 24

trainer:
  max_epochs: 15
  check_val_every_n_epoch: 1
  log_every_n_steps: 100
  devices: 1
  accelerator: gpu
  precision: 32
  gradient_clip_val: 0.5

logging:
  path: lightning_logs
  name: nbeatsaq-hier-monotone

checkpoint:
  save_top_k: 2
  ckpt_path: null
  resume_ckpt: null

random.seed: !!python/tuple [0,1,2,3,4,5,6,7]

# USAGE:
# python run.py --config=config/nbeatsaq-hier-monotone-fixed.yaml
#
# MONITORING:
# Watch train/monotone_loss - should decrease over time
# If it stays high, increase monotone_weight gradually
# If CRPS explodes, decrease monotone_weight